{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import gym\n",
    "from gym.wrappers import Monitor\n",
    "import itertools\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import sys\n",
    "import tensorflow as tf\n",
    "\n",
    "if \"../\" not in sys.path:\n",
    "  sys.path.append(\"../\")\n",
    "\n",
    "from lib import plotting\n",
    "from collections import deque, namedtuple"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "2021-09-02 11:22:33.255254: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2021-09-02 11:22:33.255410: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "metadata": {
    "collapsed": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "# atari_py import error 해결. 프로젝트 폴더에 ROMS.zip, HC ROMS.zip 파일 2개 저장 후 아래 실행\n",
    "!python -m atari_py.import_roms ."
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "copying adventure.bin from ROMS/Adventure (1980) (Atari, Warren Robinett) (CX2613, CX2613P) (PAL).bin to /home/sungbae/.local/share/virtualenvs/dennybritz-reinforcement-learning-vHFc5M9T/lib/python3.8/site-packages/atari_py/atari_roms/adventure.bin\n",
      "copying air_raid.bin from ROMS/Air Raid (Men-A-Vision) (PAL) ~.bin to /home/sungbae/.local/share/virtualenvs/dennybritz-reinforcement-learning-vHFc5M9T/lib/python3.8/site-packages/atari_py/atari_roms/air_raid.bin\n",
      "copying alien.bin from ROMS/Alien (1982) (20th Century Fox Video Games, Douglas 'Dallas North' Neubauer) (11006) ~.bin to /home/sungbae/.local/share/virtualenvs/dennybritz-reinforcement-learning-vHFc5M9T/lib/python3.8/site-packages/atari_py/atari_roms/alien.bin\n",
      "copying amidar.bin from ROMS/Amidar (1982) (Parker Brothers, Ed Temple) (PB5310) ~.bin to /home/sungbae/.local/share/virtualenvs/dennybritz-reinforcement-learning-vHFc5M9T/lib/python3.8/site-packages/atari_py/atari_roms/amidar.bin\n",
      "copying assault.bin from ROMS/Assault (AKA Sky Alien) (1983) (Bomb - Onbase) (CA281).bin to /home/sungbae/.local/share/virtualenvs/dennybritz-reinforcement-learning-vHFc5M9T/lib/python3.8/site-packages/atari_py/atari_roms/assault.bin\n",
      "copying asterix.bin from ROMS/Asterix (AKA Taz) (07-27-1983) (Atari, Jerome Domurat, Steve Woita) (CX2696) (Prototype).bin to /home/sungbae/.local/share/virtualenvs/dennybritz-reinforcement-learning-vHFc5M9T/lib/python3.8/site-packages/atari_py/atari_roms/asterix.bin\n",
      "copying asteroids.bin from ROMS/Asteroids (1981) (Atari, Brad Stewart - Sears) (CX2649 - 49-75163) [no copyright] ~.bin to /home/sungbae/.local/share/virtualenvs/dennybritz-reinforcement-learning-vHFc5M9T/lib/python3.8/site-packages/atari_py/atari_roms/asteroids.bin\n",
      "copying atlantis.bin from ROMS/Atlantis (Lost City of Atlantis) (1982) (Imagic, Dennis Koble) (720103-1A, 720103-1B, IA3203, IX-010-04) ~.bin to /home/sungbae/.local/share/virtualenvs/dennybritz-reinforcement-learning-vHFc5M9T/lib/python3.8/site-packages/atari_py/atari_roms/atlantis.bin\n",
      "copying bank_heist.bin from ROMS/Bank Heist (Bonnie & Clyde, Cops 'n' Robbers, Hold-Up, Roaring 20's) (1983) (20th Century Fox Video Games, Bill Aspromonte) (11012) ~.bin to /home/sungbae/.local/share/virtualenvs/dennybritz-reinforcement-learning-vHFc5M9T/lib/python3.8/site-packages/atari_py/atari_roms/bank_heist.bin\n",
      "copying battle_zone.bin from ROMS/Battlezone (1983) (Atari - GCC, Mike Feinstein, Brad Rice) (CX2681) ~.bin to /home/sungbae/.local/share/virtualenvs/dennybritz-reinforcement-learning-vHFc5M9T/lib/python3.8/site-packages/atari_py/atari_roms/battle_zone.bin\n",
      "copying beam_rider.bin from ROMS/Beamrider (1984) (Activision - Cheshire Engineering, David Rolfe, Larry Zwick) (AZ-037-04) ~.bin to /home/sungbae/.local/share/virtualenvs/dennybritz-reinforcement-learning-vHFc5M9T/lib/python3.8/site-packages/atari_py/atari_roms/beam_rider.bin\n",
      "copying berzerk.bin from ROMS/Berzerk (1982) (Atari, Dan Hitchens - Sears) (CX2650 - 49-75168) ~.bin to /home/sungbae/.local/share/virtualenvs/dennybritz-reinforcement-learning-vHFc5M9T/lib/python3.8/site-packages/atari_py/atari_roms/berzerk.bin\n",
      "copying bowling.bin from ROMS/Bowling (1979) (Atari, Larry Kaplan - Sears) (CX2628 - 6-99842, 49-75117) ~.bin to /home/sungbae/.local/share/virtualenvs/dennybritz-reinforcement-learning-vHFc5M9T/lib/python3.8/site-packages/atari_py/atari_roms/bowling.bin\n",
      "copying boxing.bin from ROMS/Boxing - La Boxe (1980) (Activision, Bob Whitehead) (AG-002, CAG-002, AG-002-04) ~.bin to /home/sungbae/.local/share/virtualenvs/dennybritz-reinforcement-learning-vHFc5M9T/lib/python3.8/site-packages/atari_py/atari_roms/boxing.bin\n",
      "copying breakout.bin from ROMS/Breakout - Breakaway IV (Paddle) (1978) (Atari, Brad Stewart - Sears) (CX2622 - 6-99813, 49-75107) ~.bin to /home/sungbae/.local/share/virtualenvs/dennybritz-reinforcement-learning-vHFc5M9T/lib/python3.8/site-packages/atari_py/atari_roms/breakout.bin\n",
      "copying carnival.bin from ROMS/Carnival (1982) (Coleco - Woodside Design Associates, Steve 'Jessica Stevens' Kitchen) (2468) ~.bin to /home/sungbae/.local/share/virtualenvs/dennybritz-reinforcement-learning-vHFc5M9T/lib/python3.8/site-packages/atari_py/atari_roms/carnival.bin\n",
      "copying centipede.bin from ROMS/Centipede (1983) (Atari - GCC) (CX2676) ~.bin to /home/sungbae/.local/share/virtualenvs/dennybritz-reinforcement-learning-vHFc5M9T/lib/python3.8/site-packages/atari_py/atari_roms/centipede.bin\n",
      "copying chopper_command.bin from ROMS/Chopper Command (1982) (Activision, Bob Whitehead) (AX-015, AX-015-04) ~.bin to /home/sungbae/.local/share/virtualenvs/dennybritz-reinforcement-learning-vHFc5M9T/lib/python3.8/site-packages/atari_py/atari_roms/chopper_command.bin\n",
      "copying crazy_climber.bin from ROMS/Crazy Climber (1983) (Atari - Roklan, Joe Gaucher, Alex Leavens) (CX2683) ~.bin to /home/sungbae/.local/share/virtualenvs/dennybritz-reinforcement-learning-vHFc5M9T/lib/python3.8/site-packages/atari_py/atari_roms/crazy_climber.bin\n",
      "copying defender.bin from ROMS/Defender (1982) (Atari, Robert C. Polaro, Alan J. Murphy - Sears) (CX2609 - 49-75186) ~.bin to /home/sungbae/.local/share/virtualenvs/dennybritz-reinforcement-learning-vHFc5M9T/lib/python3.8/site-packages/atari_py/atari_roms/defender.bin\n",
      "copying demon_attack.bin from ROMS/Demon Attack (Death from Above) (1982) (Imagic, Rob Fulop) (720000-200, 720101-1B, 720101-1C, IA3200, IA3200C, IX-006-04) ~.bin to /home/sungbae/.local/share/virtualenvs/dennybritz-reinforcement-learning-vHFc5M9T/lib/python3.8/site-packages/atari_py/atari_roms/demon_attack.bin\n",
      "copying donkey_kong.bin from ROMS/Donkey Kong (1982) (Coleco - Woodside Design Associates - Imaginative Systems Software, Garry Kitchen) (2451) ~.bin to /home/sungbae/.local/share/virtualenvs/dennybritz-reinforcement-learning-vHFc5M9T/lib/python3.8/site-packages/atari_py/atari_roms/donkey_kong.bin\n",
      "copying double_dunk.bin from ROMS/Double Dunk (Super Basketball) (1989) (Atari, Matthew L. Hubbard) (CX26159) ~.bin to /home/sungbae/.local/share/virtualenvs/dennybritz-reinforcement-learning-vHFc5M9T/lib/python3.8/site-packages/atari_py/atari_roms/double_dunk.bin\n",
      "copying elevator_action.bin from ROMS/Elevator Action (1983) (Atari, Dan Hitchens) (CX26126) (Prototype) ~.bin to /home/sungbae/.local/share/virtualenvs/dennybritz-reinforcement-learning-vHFc5M9T/lib/python3.8/site-packages/atari_py/atari_roms/elevator_action.bin\n",
      "copying enduro.bin from ROMS/Enduro (1983) (Activision, Larry Miller) (AX-026, AX-026-04) ~.bin to /home/sungbae/.local/share/virtualenvs/dennybritz-reinforcement-learning-vHFc5M9T/lib/python3.8/site-packages/atari_py/atari_roms/enduro.bin\n",
      "copying fishing_derby.bin from ROMS/Fishing Derby (1980) (Activision, David Crane) (AG-004) ~.bin to /home/sungbae/.local/share/virtualenvs/dennybritz-reinforcement-learning-vHFc5M9T/lib/python3.8/site-packages/atari_py/atari_roms/fishing_derby.bin\n",
      "copying freeway.bin from ROMS/Freeway (1981) (Activision, David Crane) (AG-009, AG-009-04) ~.bin to /home/sungbae/.local/share/virtualenvs/dennybritz-reinforcement-learning-vHFc5M9T/lib/python3.8/site-packages/atari_py/atari_roms/freeway.bin\n",
      "copying frogger.bin from ROMS/Frogger (1982) (Parker Brothers, Ed English, David Lamkins) (PB5300) ~.bin to /home/sungbae/.local/share/virtualenvs/dennybritz-reinforcement-learning-vHFc5M9T/lib/python3.8/site-packages/atari_py/atari_roms/frogger.bin\n",
      "copying frostbite.bin from ROMS/Frostbite (1983) (Activision, Steve Cartwright) (AX-031) ~.bin to /home/sungbae/.local/share/virtualenvs/dennybritz-reinforcement-learning-vHFc5M9T/lib/python3.8/site-packages/atari_py/atari_roms/frostbite.bin\n",
      "copying galaxian.bin from ROMS/Galaxian (1983) (Atari - GCC, Mark Ackerman, Tom Calderwood, Glenn Parker) (CX2684) ~.bin to /home/sungbae/.local/share/virtualenvs/dennybritz-reinforcement-learning-vHFc5M9T/lib/python3.8/site-packages/atari_py/atari_roms/galaxian.bin\n",
      "copying gopher.bin from ROMS/Gopher (Gopher Attack) (1982) (U.S. Games Corporation - JWDA, Sylvia Day, Todd Marshall, Robin McDaniel, Henry Will IV) (VC2001) ~.bin to /home/sungbae/.local/share/virtualenvs/dennybritz-reinforcement-learning-vHFc5M9T/lib/python3.8/site-packages/atari_py/atari_roms/gopher.bin\n",
      "copying gravitar.bin from ROMS/Gravitar (1983) (Atari, Dan Hitchens, Mimi Nyden) (CX2685) ~.bin to /home/sungbae/.local/share/virtualenvs/dennybritz-reinforcement-learning-vHFc5M9T/lib/python3.8/site-packages/atari_py/atari_roms/gravitar.bin\n",
      "copying hero.bin from ROMS/H.E.R.O. (1984) (Activision, John Van Ryzin) (AZ-036-04) ~.bin to /home/sungbae/.local/share/virtualenvs/dennybritz-reinforcement-learning-vHFc5M9T/lib/python3.8/site-packages/atari_py/atari_roms/hero.bin\n",
      "copying ice_hockey.bin from ROMS/Ice Hockey - Le Hockey Sur Glace (1981) (Activision, Alan Miller) (AX-012, CAX-012, AX-012-04) ~.bin to /home/sungbae/.local/share/virtualenvs/dennybritz-reinforcement-learning-vHFc5M9T/lib/python3.8/site-packages/atari_py/atari_roms/ice_hockey.bin\n",
      "copying jamesbond.bin from ROMS/James Bond 007 (James Bond Agent 007) (1984) (Parker Brothers - On-Time Software, Joe Gaucher, Louis Marbel) (PB5110) ~.bin to /home/sungbae/.local/share/virtualenvs/dennybritz-reinforcement-learning-vHFc5M9T/lib/python3.8/site-packages/atari_py/atari_roms/jamesbond.bin\n",
      "copying journey_escape.bin from ROMS/Journey Escape (1983) (Data Age, J. Ray Dettling) (112-006) ~.bin to /home/sungbae/.local/share/virtualenvs/dennybritz-reinforcement-learning-vHFc5M9T/lib/python3.8/site-packages/atari_py/atari_roms/journey_escape.bin\n",
      "copying kaboom.bin from ROMS/Kaboom! (Paddle) (1981) (Activision, Larry Kaplan, David Crane) (AG-010, AG-010-04) ~.bin to /home/sungbae/.local/share/virtualenvs/dennybritz-reinforcement-learning-vHFc5M9T/lib/python3.8/site-packages/atari_py/atari_roms/kaboom.bin\n",
      "copying kangaroo.bin from ROMS/Kangaroo (1983) (Atari - GCC, Kevin Osborn) (CX2689) ~.bin to /home/sungbae/.local/share/virtualenvs/dennybritz-reinforcement-learning-vHFc5M9T/lib/python3.8/site-packages/atari_py/atari_roms/kangaroo.bin\n",
      "copying keystone_kapers.bin from ROMS/Keystone Kapers - Raueber und Gendarm (1983) (Activision, Garry Kitchen - Ariola) (EAX-025, EAX-025-04I - 711 025-725) (PAL).bin to /home/sungbae/.local/share/virtualenvs/dennybritz-reinforcement-learning-vHFc5M9T/lib/python3.8/site-packages/atari_py/atari_roms/keystone_kapers.bin\n",
      "copying king_kong.bin from ROMS/King Kong (1982) (Tigervision - Software Electronics Corporation, Karl T. Olinger - Teldec) (7-001 - 3.60001 VE) (PAL).bin to /home/sungbae/.local/share/virtualenvs/dennybritz-reinforcement-learning-vHFc5M9T/lib/python3.8/site-packages/atari_py/atari_roms/king_kong.bin\n",
      "copying koolaid.bin from ROMS/Kool-Aid Man (Kool Aid Pitcher Man) (1983) (M Network, Stephen Tatsumi, Jane Terjung - Kool Aid) (MT4648) ~.bin to /home/sungbae/.local/share/virtualenvs/dennybritz-reinforcement-learning-vHFc5M9T/lib/python3.8/site-packages/atari_py/atari_roms/koolaid.bin\n",
      "copying krull.bin from ROMS/Krull (1983) (Atari, Jerome Domurat, Dave Staugas) (CX2682) ~.bin to /home/sungbae/.local/share/virtualenvs/dennybritz-reinforcement-learning-vHFc5M9T/lib/python3.8/site-packages/atari_py/atari_roms/krull.bin\n",
      "copying kung_fu_master.bin from ROMS/Kung-Fu Master (1987) (Activision - Imagineering, Dan Kitchen, Garry Kitchen) (AG-039-04) ~.bin to /home/sungbae/.local/share/virtualenvs/dennybritz-reinforcement-learning-vHFc5M9T/lib/python3.8/site-packages/atari_py/atari_roms/kung_fu_master.bin\n",
      "copying laser_gates.bin from ROMS/Laser Gates (AKA Innerspace) (1983) (Imagic, Dan Oliver) (720118-2A, 13208, EIX-007-04I) (PAL).bin to /home/sungbae/.local/share/virtualenvs/dennybritz-reinforcement-learning-vHFc5M9T/lib/python3.8/site-packages/atari_py/atari_roms/laser_gates.bin\n",
      "copying lost_luggage.bin from ROMS/Lost Luggage (Airport Mayhem) (1982) (Apollo - Games by Apollo, Larry Minor, Ernie Runyon, Ed Salvo) (AP-2004) [no opening scene] ~.bin to /home/sungbae/.local/share/virtualenvs/dennybritz-reinforcement-learning-vHFc5M9T/lib/python3.8/site-packages/atari_py/atari_roms/lost_luggage.bin\n",
      "copying montezuma_revenge.bin from ROMS/Montezuma's Revenge - Featuring Panama Joe (1984) (Parker Brothers - JWDA, Henry Will IV) (PB5760) ~.bin to /home/sungbae/.local/share/virtualenvs/dennybritz-reinforcement-learning-vHFc5M9T/lib/python3.8/site-packages/atari_py/atari_roms/montezuma_revenge.bin\n",
      "copying mr_do.bin from ROMS/Mr. Do! (1983) (CBS Electronics, Ed English) (4L4478) (PAL).bin to /home/sungbae/.local/share/virtualenvs/dennybritz-reinforcement-learning-vHFc5M9T/lib/python3.8/site-packages/atari_py/atari_roms/mr_do.bin\n",
      "copying ms_pacman.bin from ROMS/Ms. Pac-Man (1983) (Atari - GCC, Mark Ackerman, Glenn Parker) (CX2675) ~.bin to /home/sungbae/.local/share/virtualenvs/dennybritz-reinforcement-learning-vHFc5M9T/lib/python3.8/site-packages/atari_py/atari_roms/ms_pacman.bin\n",
      "copying name_this_game.bin from ROMS/Name This Game (Guardians of Treasure) (1983) (U.S. Games Corporation - JWDA, Roger Booth, Sylvia Day, Ron Dubren, Todd Marshall, Robin McDaniel, Wes Trager, Henry Will IV) (VC1007) ~.bin to /home/sungbae/.local/share/virtualenvs/dennybritz-reinforcement-learning-vHFc5M9T/lib/python3.8/site-packages/atari_py/atari_roms/name_this_game.bin\n",
      "copying pacman.bin from ROMS/Pac-Man (1982) (Atari, Tod Frye) (CX2646) (PAL).bin to /home/sungbae/.local/share/virtualenvs/dennybritz-reinforcement-learning-vHFc5M9T/lib/python3.8/site-packages/atari_py/atari_roms/pacman.bin\n",
      "copying phoenix.bin from ROMS/Phoenix (1983) (Atari - GCC, Mike Feinstein, John Mracek) (CX2673) ~.bin to /home/sungbae/.local/share/virtualenvs/dennybritz-reinforcement-learning-vHFc5M9T/lib/python3.8/site-packages/atari_py/atari_roms/phoenix.bin\n",
      "copying video_pinball.bin from ROMS/Pinball (AKA Video Pinball) (Zellers).bin to /home/sungbae/.local/share/virtualenvs/dennybritz-reinforcement-learning-vHFc5M9T/lib/python3.8/site-packages/atari_py/atari_roms/video_pinball.bin\n",
      "copying pitfall.bin from ROMS/Pitfall! - Pitfall Harry's Jungle Adventure (Jungle Runner) (1982) (Activision, David Crane) (AX-018, AX-018-04) ~.bin to /home/sungbae/.local/share/virtualenvs/dennybritz-reinforcement-learning-vHFc5M9T/lib/python3.8/site-packages/atari_py/atari_roms/pitfall.bin\n",
      "copying pooyan.bin from ROMS/Pooyan (1983) (Konami) (RC 100-X 02) ~.bin to /home/sungbae/.local/share/virtualenvs/dennybritz-reinforcement-learning-vHFc5M9T/lib/python3.8/site-packages/atari_py/atari_roms/pooyan.bin\n",
      "copying private_eye.bin from ROMS/Private Eye (1984) (Activision, Bob Whitehead) (AG-034-04) ~.bin to /home/sungbae/.local/share/virtualenvs/dennybritz-reinforcement-learning-vHFc5M9T/lib/python3.8/site-packages/atari_py/atari_roms/private_eye.bin\n",
      "copying qbert.bin from ROMS/Q-bert (1983) (Parker Brothers - Western Technologies, Dave Hampton, Tom Sloper) (PB5360) ~.bin to /home/sungbae/.local/share/virtualenvs/dennybritz-reinforcement-learning-vHFc5M9T/lib/python3.8/site-packages/atari_py/atari_roms/qbert.bin\n",
      "copying riverraid.bin from ROMS/River Raid (1982) (Activision, Carol Shaw) (AX-020, AX-020-04) ~.bin to /home/sungbae/.local/share/virtualenvs/dennybritz-reinforcement-learning-vHFc5M9T/lib/python3.8/site-packages/atari_py/atari_roms/riverraid.bin\n",
      "copying road_runner.bin from patched version of ROMS/Road Runner (1989) (Atari - Bobco, Robert C. Polaro) (CX2663) ~.bin to /home/sungbae/.local/share/virtualenvs/dennybritz-reinforcement-learning-vHFc5M9T/lib/python3.8/site-packages/atari_py/atari_roms/road_runner.bin\n",
      "copying robotank.bin from ROMS/Robot Tank (Robotank) (1983) (Activision, Alan Miller) (AZ-028, AG-028-04) ~.bin to /home/sungbae/.local/share/virtualenvs/dennybritz-reinforcement-learning-vHFc5M9T/lib/python3.8/site-packages/atari_py/atari_roms/robotank.bin\n",
      "copying seaquest.bin from ROMS/Seaquest (1983) (Activision, Steve Cartwright) (AX-022) ~.bin to /home/sungbae/.local/share/virtualenvs/dennybritz-reinforcement-learning-vHFc5M9T/lib/python3.8/site-packages/atari_py/atari_roms/seaquest.bin\n",
      "copying sir_lancelot.bin from ROMS/Sir Lancelot (1983) (Xonox - K-Tel Software - Product Guild, Anthony R. Henderson) (99006, 6220) (PAL).bin to /home/sungbae/.local/share/virtualenvs/dennybritz-reinforcement-learning-vHFc5M9T/lib/python3.8/site-packages/atari_py/atari_roms/sir_lancelot.bin\n",
      "copying skiing.bin from ROMS/Skiing - Le Ski (1980) (Activision, Bob Whitehead) (AG-005, CAG-005, AG-005-04) ~.bin to /home/sungbae/.local/share/virtualenvs/dennybritz-reinforcement-learning-vHFc5M9T/lib/python3.8/site-packages/atari_py/atari_roms/skiing.bin\n",
      "copying solaris.bin from ROMS/Solaris (The Last Starfighter, Star Raiders II, Universe) (1986) (Atari, Douglas Neubauer, Mimi Nyden) (CX26136) ~.bin to /home/sungbae/.local/share/virtualenvs/dennybritz-reinforcement-learning-vHFc5M9T/lib/python3.8/site-packages/atari_py/atari_roms/solaris.bin\n",
      "copying space_invaders.bin from ROMS/Space Invaders (1980) (Atari, Richard Maurer - Sears) (CX2632 - 49-75153) ~.bin to /home/sungbae/.local/share/virtualenvs/dennybritz-reinforcement-learning-vHFc5M9T/lib/python3.8/site-packages/atari_py/atari_roms/space_invaders.bin\n",
      "copying star_gunner.bin from ROMS/Stargunner (1983) (Telesys, Alex Leavens) (1005) ~.bin to /home/sungbae/.local/share/virtualenvs/dennybritz-reinforcement-learning-vHFc5M9T/lib/python3.8/site-packages/atari_py/atari_roms/star_gunner.bin\n",
      "copying surround.bin from ROMS/Surround (32 in 1) (Bit Corporation) (R320).bin to /home/sungbae/.local/share/virtualenvs/dennybritz-reinforcement-learning-vHFc5M9T/lib/python3.8/site-packages/atari_py/atari_roms/surround.bin\n",
      "copying tennis.bin from ROMS/Tennis - Le Tennis (1981) (Activision, Alan Miller) (AG-007, CAG-007) ~.bin to /home/sungbae/.local/share/virtualenvs/dennybritz-reinforcement-learning-vHFc5M9T/lib/python3.8/site-packages/atari_py/atari_roms/tennis.bin\n",
      "copying time_pilot.bin from ROMS/Time Pilot (1983) (Coleco - Woodside Design Associates, Harley H. Puthuff Jr.) (2663) ~.bin to /home/sungbae/.local/share/virtualenvs/dennybritz-reinforcement-learning-vHFc5M9T/lib/python3.8/site-packages/atari_py/atari_roms/time_pilot.bin\n",
      "copying trondead.bin from ROMS/TRON - Deadly Discs (TRON Joystick) (1983) (M Network - INTV - APh Technological Consulting, Jeff Ronne, Brett Stutz) (MT5662) ~.bin to /home/sungbae/.local/share/virtualenvs/dennybritz-reinforcement-learning-vHFc5M9T/lib/python3.8/site-packages/atari_py/atari_roms/trondead.bin\n",
      "copying tutankham.bin from ROMS/Tutankham (1983) (Parker Brothers, Dave Engman, Dawn Stockbridge) (PB5340) ~.bin to /home/sungbae/.local/share/virtualenvs/dennybritz-reinforcement-learning-vHFc5M9T/lib/python3.8/site-packages/atari_py/atari_roms/tutankham.bin\n",
      "copying up_n_down.bin from ROMS/Up 'n Down (1984) (SEGA - Beck-Tech, Steve Beck, Phat Ho) (009-01) ~.bin to /home/sungbae/.local/share/virtualenvs/dennybritz-reinforcement-learning-vHFc5M9T/lib/python3.8/site-packages/atari_py/atari_roms/up_n_down.bin\n",
      "copying venture.bin from ROMS/Venture (1982) (Coleco, Joseph Biel) (2457) ~.bin to /home/sungbae/.local/share/virtualenvs/dennybritz-reinforcement-learning-vHFc5M9T/lib/python3.8/site-packages/atari_py/atari_roms/venture.bin\n",
      "copying pong.bin from ROMS/Video Olympics - Pong Sports (Paddle) (1977) (Atari, Joe Decuir - Sears) (CX2621 - 99806, 6-99806, 49-75104) ~.bin to /home/sungbae/.local/share/virtualenvs/dennybritz-reinforcement-learning-vHFc5M9T/lib/python3.8/site-packages/atari_py/atari_roms/pong.bin\n",
      "copying wizard_of_wor.bin from ROMS/Wizard of Wor (1982) (CBS Electronics - Roklan, Joe Hellesen, Joe Wagner) (M8774, M8794) ~.bin to /home/sungbae/.local/share/virtualenvs/dennybritz-reinforcement-learning-vHFc5M9T/lib/python3.8/site-packages/atari_py/atari_roms/wizard_of_wor.bin\n",
      "copying yars_revenge.bin from ROMS/Yars' Revenge (Time Freeze) (1982) (Atari, Howard Scott Warshaw - Sears) (CX2655 - 49-75167) ~.bin to /home/sungbae/.local/share/virtualenvs/dennybritz-reinforcement-learning-vHFc5M9T/lib/python3.8/site-packages/atari_py/atari_roms/yars_revenge.bin\n",
      "copying zaxxon.bin from ROMS/Zaxxon (1983) (Coleco) (2454) ~.bin to /home/sungbae/.local/share/virtualenvs/dennybritz-reinforcement-learning-vHFc5M9T/lib/python3.8/site-packages/atari_py/atari_roms/zaxxon.bin\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "env = gym.envs.make(\"Breakout-v0\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "# Atari Actions: 0 (noop), 1 (fire), 2 (left) and 3 (right) are valid actions\n",
    "VALID_ACTIONS = [0, 1, 2, 3]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "class StateProcessor():\n",
    "    \"\"\"\n",
    "    Processes a raw Atari images. Resizes it and converts it to grayscale.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        # Build the Tensorflow graph\n",
    "        with tf.variable_scope(\"state_processor\"): # 이 안에서 생성되는 변수들은 \"state_processor/변수명\"으로 명명됨.\n",
    "            self.input_state = tf.placeholder(shape=[210, 160, 3], dtype=tf.uint8) # atari RGB 형태인 [210, 160, 3]을 state 형태로 설정\n",
    "            self.output = tf.image.rgb_to_grayscale(self.input_state) # 입력된 이미지를 RGB에서 gray scale로 변환\n",
    "            self.output = tf.image.crop_to_bounding_box(self.output, 34, 0, 160, 160) # 이미지 잘라내기\n",
    "            self.output = tf.image.resize_images( # 이미지 크기 조정\n",
    "                self.output, [84, 84], method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
    "            self.output = tf.squeeze(self.output) # 크기 1인 차원을 제거\n",
    "\n",
    "    def process(self, sess, state):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            sess: A Tensorflow session object\n",
    "            state: A [210, 160, 3] Atari RGB State\n",
    "\n",
    "        Returns:\n",
    "            A processed [84, 84] state representing grayscale values.\n",
    "        \"\"\"\n",
    "        return sess.run(self.output, { self.input_state: state }) # 위에서 정의한 input -> output 과정을 진행"
   ],
   "outputs": [],
   "metadata": {
    "collapsed": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "class Estimator():\n",
    "    \"\"\"Q-Value Estimator neural network.\n",
    "\n",
    "    This network is used for both the Q-Network and the Target Network.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, scope=\"estimator\", summaries_dir=None):\n",
    "        self.scope = scope\n",
    "        # Writes Tensorboard summaries to disk\n",
    "        self.summary_writer = None\n",
    "        with tf.variable_scope(scope):\n",
    "            # Build the graph\n",
    "            self._build_model() # Estimator()를 호출할 때 아래 _build_model 함수에 따라 q value prediction 및 error 계산을 위한 model 생성\n",
    "            if summaries_dir:\n",
    "                summary_dir = os.path.join(summaries_dir, \"summaries_{}\".format(scope))\n",
    "                if not os.path.exists(summary_dir):\n",
    "                    os.makedirs(summary_dir)\n",
    "                self.summary_writer = tf.summary.FileWriter(summary_dir)\n",
    "\n",
    "    def _build_model(self):\n",
    "        \"\"\"\n",
    "        Builds the Tensorflow graph.\n",
    "        \"\"\"\n",
    "\n",
    "        # Placeholders for our input\n",
    "\n",
    "        # Our input are 4 grayscale frames of shape 84, 84 each\n",
    "        self.X_pl = tf.placeholder(shape=[None, 84, 84, 4], dtype=tf.uint8, name=\"X\")\n",
    "        # The TD target value\n",
    "        self.y_pl = tf.placeholder(shape=[None], dtype=tf.float32, name=\"y\")\n",
    "        # Integer id of which action was selected\n",
    "        self.actions_pl = tf.placeholder(shape=[None], dtype=tf.int32, name=\"actions\")\n",
    "\n",
    "        X = tf.to_float(self.X_pl) / 255.0\n",
    "        batch_size = tf.shape(self.X_pl)[0]\n",
    "\n",
    "        # Three convolutional layers\n",
    "        conv1 = tf.contrib.layers.conv2d(\n",
    "            inputs=X, num_outputs=32, kernel_size=8, stride=4, activation_fn=tf.nn.relu) # num_outputs = channel 수  \n",
    "        conv2 = tf.contrib.layers.conv2d(\n",
    "            inputs=conv1, num_outputs=64, kernel_size=4, stride=2, activation_fn=tf.nn.relu)\n",
    "        conv3 = tf.contrib.layers.conv2d(\n",
    "            inputs=conv2, num_outputs=64, kernel_size=3, stride=1, activation_fn=tf.nn.relu)\n",
    "\n",
    "        # Fully connected layers\n",
    "        flattened = tf.contrib.layers.flatten(conv3)\n",
    "        fc1 = tf.contrib.layers.fully_connected(flattened, 512)\n",
    "        self.predictions = tf.contrib.layers.fully_connected(fc1, len(VALID_ACTIONS))\n",
    "\n",
    "\n",
    "        # Get the predictions for the chosen actions only\n",
    "        gather_indices = tf.range(batch_size) * tf.shape(self.predictions)[1] + self.actions_pl\n",
    "        self.action_predictions = tf.gather(tf.reshape(self.predictions, [-1]), gather_indices)\n",
    "\n",
    "        # Calculate the loss\n",
    "        self.losses = tf.squared_difference(self.y_pl, self.action_predictions) # target과 prediction의 차이\n",
    "        self.loss = tf.reduce_mean(self.losses)\n",
    "\n",
    "        # Optimizer Parameters from original paper\n",
    "        self.optimizer = tf.train.RMSPropOptimizer(0.00025, 0.99, 0.0, 1e-6)\n",
    "        self.train_op = self.optimizer.minimize(self.loss, global_step=tf.contrib.framework.get_global_step())\n",
    "\n",
    "        # Summaries for Tensorboard\n",
    "        self.summaries = tf.summary.merge([\n",
    "            tf.summary.scalar(\"loss\", self.loss),\n",
    "            tf.summary.histogram(\"loss_hist\", self.losses),\n",
    "            tf.summary.histogram(\"q_values_hist\", self.predictions),\n",
    "            tf.summary.scalar(\"max_q_value\", tf.reduce_max(self.predictions))\n",
    "        ])\n",
    "\n",
    "\n",
    "    def predict(self, sess, s):\n",
    "        \"\"\"\n",
    "        Predicts action values.\n",
    "\n",
    "        Args:\n",
    "          sess: Tensorflow session\n",
    "          s: State input of shape [batch_size, 4, 84, 84, 1]\n",
    "\n",
    "        Returns:\n",
    "          Tensor of shape [batch_size, NUM_VALID_ACTIONS] containing the estimated \n",
    "          action values.\n",
    "        \"\"\"\n",
    "        return sess.run(self.predictions, { self.X_pl: s })  # 위 과정대로 생성한 모델을 통해 state가 입력되면 action 별 q value 예측값 반환\n",
    "\n",
    "    def update(self, sess, s, a, y):\n",
    "        \"\"\"\n",
    "        Updates the estimator towards the given targets.\n",
    "\n",
    "        Args:\n",
    "          sess: Tensorflow session object\n",
    "          s: State input of shape [batch_size, 4, 84, 84, 1]\n",
    "          a: Chosen actions of shape [batch_size]\n",
    "          y: Targets of shape [batch_size]\n",
    "\n",
    "        Returns:\n",
    "          The calculated loss on the batch.\n",
    "        \"\"\"\n",
    "        feed_dict = { self.X_pl: s, self.y_pl: y, self.actions_pl: a }\n",
    "        summaries, global_step, _, loss = sess.run(\n",
    "            [self.summaries, tf.contrib.framework.get_global_step(), self.train_op, self.loss],\n",
    "            feed_dict)\n",
    "        if self.summary_writer:\n",
    "            self.summary_writer.add_summary(summaries, global_step)\n",
    "        return loss"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# For Testing....\n",
    "\n",
    "tf.reset_default_graph()\n",
    "global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n",
    "\n",
    "e = Estimator(scope=\"test\") # e에 test를 위한 model을 구성\n",
    "sp = StateProcessor()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Example observation batch\n",
    "    observation = env.reset()\n",
    "    \n",
    "    observation_p = sp.process(sess, observation) # 환경의 obeservation을 그대로 사용하는 것이 아니라, StateProcessor()를 통해 처리한 뒤 사용\n",
    "    observation = np.stack([observation_p] * 4, axis=2)\n",
    "    observations = np.array([observation] * 2)\n",
    "    \n",
    "    # Test Prediction\n",
    "    print(e.predict(sess, observations)) # observation(state)에 대한 각 action 의 estimated value 산출\n",
    "\n",
    "    # Test training step\n",
    "    y = np.array([10.0, 10.0])\n",
    "    a = np.array([1, 3])\n",
    "    print(e.update(sess, observations, a, y)) # action 1과 3에 대한 model update"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def copy_model_parameters(sess, estimator1, estimator2):\n",
    "    \"\"\"\n",
    "    Copies the model parameters of one estimator to another.\n",
    "\n",
    "    Args:\n",
    "      sess: Tensorflow session instance\n",
    "      estimator1: Estimator to copy the paramters from\n",
    "      estimator2: Estimator to copy the parameters to\n",
    "    \"\"\"\n",
    "    e1_params = [t for t in tf.trainable_variables() if t.name.startswith(estimator1.scope)]\n",
    "    e1_params = sorted(e1_params, key=lambda v: v.name)\n",
    "    e2_params = [t for t in tf.trainable_variables() if t.name.startswith(estimator2.scope)]\n",
    "    e2_params = sorted(e2_params, key=lambda v: v.name)\n",
    "\n",
    "    update_ops = []\n",
    "    for e1_v, e2_v in zip(e1_params, e2_params):\n",
    "        op = e2_v.assign(e1_v)\n",
    "        update_ops.append(op)\n",
    "\n",
    "    sess.run(update_ops)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def make_epsilon_greedy_policy(estimator, nA):\n",
    "    \"\"\"\n",
    "    Creates an epsilon-greedy policy based on a given Q-function approximator and epsilon.\n",
    "\n",
    "    Args:\n",
    "        estimator: An estimator that returns q values for a given state\n",
    "        nA: Number of actions in the environment.\n",
    "\n",
    "    Returns:\n",
    "        A function that takes the (sess, observation, epsilon) as an argument and returns\n",
    "        the probabilities for each action in the form of a numpy array of length nA.\n",
    "\n",
    "    \"\"\"\n",
    "    def policy_fn(sess, observation, epsilon):\n",
    "        A = np.ones(nA, dtype=float) * epsilon / nA\n",
    "        q_values = estimator.predict(sess, np.expand_dims(observation, 0))[0]\n",
    "        best_action = np.argmax(q_values)\n",
    "        A[best_action] += (1.0 - epsilon)\n",
    "        return A\n",
    "    return policy_fn"
   ],
   "outputs": [],
   "metadata": {
    "collapsed": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def deep_q_learning(sess,\n",
    "                    env,\n",
    "                    q_estimator,\n",
    "                    target_estimator,\n",
    "                    state_processor,\n",
    "                    num_episodes,\n",
    "                    experiment_dir,\n",
    "                    replay_memory_size=500000,\n",
    "                    replay_memory_init_size=50000,\n",
    "                    update_target_estimator_every=10000,\n",
    "                    discount_factor=0.99,\n",
    "                    epsilon_start=1.0,\n",
    "                    epsilon_end=0.1,\n",
    "                    epsilon_decay_steps=500000,\n",
    "                    batch_size=32,\n",
    "                    record_video_every=50):\n",
    "    \"\"\"\n",
    "    Q-Learning algorithm for off-policy TD control using Function Approximation.\n",
    "    Finds the optimal greedy policy while following an epsilon-greedy policy.\n",
    "\n",
    "    Args:\n",
    "        sess: Tensorflow Session object\n",
    "        env: OpenAI environment\n",
    "        q_estimator: Estimator object used for the q values\n",
    "        target_estimator: Estimator object used for the targets\n",
    "        state_processor: A StateProcessor object\n",
    "        num_episodes: Number of episodes to run for\n",
    "        experiment_dir: Directory to save Tensorflow summaries in\n",
    "        replay_memory_size: Size of the replay memory\n",
    "        replay_memory_init_size: Number of random experiences to sampel when initializing \n",
    "          the reply memory.\n",
    "        update_target_estimator_every: Copy parameters from the Q estimator to the \n",
    "          target estimator every N steps\n",
    "        discount_factor: Gamma discount factor\n",
    "        epsilon_start: Chance to sample a random action when taking an action.\n",
    "          Epsilon is decayed over time and this is the start value\n",
    "        epsilon_end: The final minimum value of epsilon after decaying is done\n",
    "        epsilon_decay_steps: Number of steps to decay epsilon over\n",
    "        batch_size: Size of batches to sample from the replay memory\n",
    "        record_video_every: Record a video every N episodes\n",
    "\n",
    "    Returns:\n",
    "        An EpisodeStats object with two numpy arrays for episode_lengths and episode_rewards.\n",
    "    \"\"\"\n",
    "\n",
    "    Transition = namedtuple(\"Transition\", [\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
    "\n",
    "    # The replay memory\n",
    "    replay_memory = []\n",
    "\n",
    "    # Keeps track of useful statistics\n",
    "    stats = plotting.EpisodeStats(\n",
    "        episode_lengths=np.zeros(num_episodes),\n",
    "        episode_rewards=np.zeros(num_episodes))\n",
    "\n",
    "    # Create directories for checkpoints and summaries\n",
    "    checkpoint_dir = os.path.join(experiment_dir, \"checkpoints\")\n",
    "    checkpoint_path = os.path.join(checkpoint_dir, \"model\")\n",
    "    monitor_path = os.path.join(experiment_dir, \"monitor\")\n",
    "\n",
    "    if not os.path.exists(checkpoint_dir):\n",
    "        os.makedirs(checkpoint_dir)\n",
    "    if not os.path.exists(monitor_path):\n",
    "        os.makedirs(monitor_path)\n",
    "\n",
    "    saver = tf.train.Saver()\n",
    "    # Load a previous checkpoint if we find one\n",
    "    latest_checkpoint = tf.train.latest_checkpoint(checkpoint_dir)\n",
    "    if latest_checkpoint:\n",
    "        print(\"Loading model checkpoint {}...\\n\".format(latest_checkpoint))\n",
    "        saver.restore(sess, latest_checkpoint)\n",
    "    \n",
    "    # Get the current time step\n",
    "    total_t = sess.run(tf.contrib.framework.get_global_step())\n",
    "\n",
    "    # The epsilon decay schedule\n",
    "    epsilons = np.linspace(epsilon_start, epsilon_end, epsilon_decay_steps)\n",
    "\n",
    "    # The policy we're following\n",
    "    policy = make_epsilon_greedy_policy(\n",
    "        q_estimator,\n",
    "        len(VALID_ACTIONS))\n",
    "\n",
    "    # Populate the replay memory with initial experience\n",
    "    print(\"Populating replay memory...\")\n",
    "    state = env.reset()\n",
    "    state = state_processor.process(sess, state)\n",
    "    state = np.stack([state] * 4, axis=2)\n",
    "    for i in range(replay_memory_init_size):\n",
    "        # TODO: Populate replay memory!\n",
    "        pass\n",
    "\n",
    "    # Record videos\n",
    "    env= Monitor(env,\n",
    "                 directory=monitor_path,\n",
    "                 resume=True,\n",
    "                 video_callable=lambda count: count % record_video_every == 0)\n",
    "\n",
    "    for i_episode in range(num_episodes):\n",
    "\n",
    "        # Save the current checkpoint\n",
    "        saver.save(tf.get_default_session(), checkpoint_path)\n",
    "\n",
    "        # Reset the environment\n",
    "        state = env.reset()\n",
    "        state = state_processor.process(sess, state)\n",
    "        state = np.stack([state] * 4, axis=2)\n",
    "        loss = None\n",
    "\n",
    "        # One step in the environment\n",
    "        for t in itertools.count():\n",
    "\n",
    "            # Epsilon for this time step\n",
    "            epsilon = epsilons[min(total_t, epsilon_decay_steps-1)]\n",
    "\n",
    "            # Add epsilon to Tensorboard\n",
    "            episode_summary = tf.Summary()\n",
    "            episode_summary.value.add(simple_value=epsilon, tag=\"epsilon\")\n",
    "            q_estimator.summary_writer.add_summary(episode_summary, total_t)\n",
    "\n",
    "            # TODO: Maybe update the target estimator\n",
    "            if total_t % update_target_estimator_every == 0:\n",
    "                pass\n",
    "\n",
    "            # Print out which step we're on, useful for debugging.\n",
    "            print(\"\\rStep {} ({}) @ Episode {}/{}, loss: {}\".format(\n",
    "                    t, total_t, i_episode + 1, num_episodes, loss), end=\"\")\n",
    "            sys.stdout.flush()\n",
    "\n",
    "            # Take a step in the environment\n",
    "            # TODO: Implement!\n",
    "\n",
    "            # If our replay memory is full, pop the first element\n",
    "            if len(replay_memory) == replay_memory_size:\n",
    "                replay_memory.pop(0)\n",
    "\n",
    "            # TODO: Save transition to replay memory\n",
    "\n",
    "            # Update statistics\n",
    "            stats.episode_rewards[i_episode] += reward\n",
    "            stats.episode_lengths[i_episode] = t\n",
    "\n",
    "            # TODO: Sample a minibatch from the replay memory\n",
    "            # TODO: Calculate q values and targets\n",
    "            # TODO Perform gradient descent update\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "            state = next_state\n",
    "            total_t += 1\n",
    "\n",
    "        # Add summaries to tensorboard\n",
    "        episode_summary = tf.Summary()\n",
    "        episode_summary.value.add(simple_value=stats.episode_rewards[i_episode], node_name=\"episode_reward\", tag=\"episode_reward\")\n",
    "        episode_summary.value.add(simple_value=stats.episode_lengths[i_episode], node_name=\"episode_length\", tag=\"episode_length\")\n",
    "        q_estimator.summary_writer.add_summary(episode_summary, total_t)\n",
    "        q_estimator.summary_writer.flush()\n",
    "\n",
    "        yield total_t, plotting.EpisodeStats(\n",
    "            episode_lengths=stats.episode_lengths[:i_episode+1],\n",
    "            episode_rewards=stats.episode_rewards[:i_episode+1])\n",
    "\n",
    "    env.monitor.close()\n",
    "    return stats"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "# Where we save our checkpoints and graphs\n",
    "experiment_dir = os.path.abspath(\"./experiments/{}\".format(env.spec.id))\n",
    "\n",
    "# Create a glboal step variable\n",
    "global_step = tf.Variable(0, name='global_step', trainable=False)\n",
    "    \n",
    "# Create estimators\n",
    "q_estimator = Estimator(scope=\"q\", summaries_dir=experiment_dir)\n",
    "target_estimator = Estimator(scope=\"target_q\")\n",
    "\n",
    "# State processor\n",
    "state_processor = StateProcessor()\n",
    "\n",
    "# Run it!\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.initialize_all_variables())\n",
    "    for t, stats in deep_q_learning(sess,\n",
    "                                    env,\n",
    "                                    q_estimator=q_estimator,\n",
    "                                    target_estimator=target_estimator,\n",
    "                                    state_processor=state_processor,\n",
    "                                    experiment_dir=experiment_dir,\n",
    "                                    num_episodes=10000,\n",
    "                                    replay_memory_size=500000,\n",
    "                                    replay_memory_init_size=50000,\n",
    "                                    update_target_estimator_every=10000,\n",
    "                                    epsilon_start=1.0,\n",
    "                                    epsilon_end=0.1,\n",
    "                                    epsilon_decay_steps=500000,\n",
    "                                    discount_factor=0.99,\n",
    "                                    batch_size=32):\n",
    "\n",
    "        print(\"\\nEpisode Reward: {}\".format(stats.episode_rewards[-1]))"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.10 64-bit ('dennybritz-reinforcement-learning-vHFc5M9T': pipenv)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "interpreter": {
   "hash": "2ec098a07e92f4ebe53dfcd804baf622d74b064a83a562e3c7b20137eab92767"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}